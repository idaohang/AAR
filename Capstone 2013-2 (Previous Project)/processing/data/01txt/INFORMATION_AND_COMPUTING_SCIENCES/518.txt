Dynamic Web content is increasing in popularity, and by its nature, is harder to scale than static content. As a result, dynamic web content delivery degrades more rapidly than static content under similar client request rates. Many techniques have been explored for effectively handling heavy Web request traffic. In this paper, we concentrate on dynamic content degradation, believing that it offers a good balance between minimizing total cost of ownership and maximizing scalability. We describe an algorithm for dynamic content degradation that is easily implementable on top of existing, mainstream web application architectures. The algorithm is based on measuring elapsed-time of content generation. We demonstrate the algorithmâ€™s adaptability against two request traffic patterns, and explore behavioural changes when varying the algorithm's key parameters. We find our elapsed-time based algorithm is better at recognizing when the server is unloaded, that the supporting architecture limits the effectiveness of the algorithm and and that the algorithm must be configured pessimistically for best results under load.
