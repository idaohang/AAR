Reinforcement Learning (RL) is a learning framework for modelling an agent and its  interaction with its environment through actions, perceptions, and rewards. Intelligent  agents should choose actions after every perception, such that their long-term reward is  maximized. A well defined framework for this interaction is the partially observable Markov decision process model (POMDP). Unfortunately solving POMDPs is an intractable  problem. To overcome the problem of partial observability, McCallum introduced the U-tree, a RL algorithm that uses selective attention and short-term memory to simultaneously address the intertwined problems of large perceptual state spaces and hidden states. A U-tree embodies the policy and the state representation of the environment of the agent. A U-tree combines the advantages of instance-based learning with robust statistical tests for separating noise from task structure. In this paper, we consider an alternative approach for the feature selection of past events for the construction of the state representation. We apply information theory and decision tree techniques to derive a variation of the U-tree. The relevance of the candidate features is assessed by ranking the information gain ratio with respect to the cumulative expected reward. Experiments carried on three different RL tasks demonstrate that our variant of the U-tree produces a more robust state representation and faster learning. This better performance can be explained by the fact that the information gain ratio exhibits a lower variance in return prediction than the Kolmogorov-Smirnov statistical test used in the original U-tree algorithm.
